{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10149,
     "status": "ok",
     "timestamp": 1649683115877,
     "user": {
      "displayName": "iseki marie",
      "userId": "09150555860364815380"
     },
     "user_tz": -540
    },
    "id": "VwQeBFaqO86_",
    "outputId": "795a5b6b-eedf-408c-f214-29b5477ad6f9"
   },
   "outputs": [],
   "source": [
    "# ライブラリのDL(termextract)\n",
    "!wget http://gensen.dl.itc.u-tokyo.ac.jp/soft/pytermextract-0_02.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1649683122580,
     "user": {
      "displayName": "iseki marie",
      "userId": "09150555860364815380"
     },
     "user_tz": -540
    },
    "id": "Ljdf0KB0RIZx",
    "outputId": "abd8c888-f9c6-420b-e110-ef9710153958"
   },
   "outputs": [],
   "source": [
    "# ライブラリの解凍\n",
    "import shutil\n",
    "shutil.unpack_archive('pytermextract-0_02.zip', 'pytermextract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1649683127867,
     "user": {
      "displayName": "iseki marie",
      "userId": "09150555860364815380"
     },
     "user_tz": -540
    },
    "id": "6EtgW9ClRP-f",
    "outputId": "ab22d14f-c771-4f14-c329-651c71e7f656"
   },
   "outputs": [],
   "source": [
    "# ライブラリのインストール\n",
    "# cd /pytermextract/\n",
    "# pytermextract> pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.スクレイピングしたhtmlから平文を抽出\n",
    "def get_body(url):\n",
    "    from bs4 import BeautifulSoup #必要なモジュールのインポート\n",
    "    import requests\n",
    "    \n",
    "    try:\n",
    "        html = requests.get(url) #Webページの取得\n",
    "    except:\n",
    "        print(\"Invalid url!\") #Webページの取得に失敗した場合の処理\n",
    "        return -1\n",
    "    soup = BeautifulSoup(html.content, \"html.parser\") #Webページからhtmlの取得\n",
    "    return soup.find(\"body\").text #htmlからbodyを取り出し、テキストのみを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関数get_bodyの結果表示\n",
    "print(get_body(\"https://www.softbank.jp/biz/ai/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.キーワード抽出（Janome利用）\n",
    "def extract_keyword(text):\n",
    "    import termextract.janome #必要なモジュールのインポート\n",
    "    import termextract.core\n",
    "    from janome.tokenizer import Tokenizer\n",
    "    import collections \n",
    "    \n",
    "    t = Tokenizer()\n",
    "    tokenize_text = t.tokenize(text) #janomeによる日本語の形態素解析の処理\n",
    "    frequency = termextract.janome.cmp_noun_dict(tokenize_text) #複合語抽出処理\n",
    "\n",
    "    lr = termextract.core.score_lr( #FrequencyからLRを生成する\n",
    "        frequency,\n",
    "        ignore_words=termextract.mecab.IGNORE_WORDS,\n",
    "        lr_mode=1, average_rate=1)\n",
    "    \n",
    "    term_imp = termextract.core.term_importance(frequency, lr) #FrequencyとLRを組み合わせ、FLRの重要度を出す\n",
    "\n",
    "    data_collection = collections.Counter(term_imp) #collectionsを使用して重要度が高い順に整理\n",
    "    return data_collection.most_common()[0][0] #もっとも重要度の高い単語を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関数extract_keywordの結果表示\n",
    "extract_keyword(get_body(\"https://www.softbank.jp/biz/ai/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.google検索の結果からスクレイピングと解析を実施\n",
    "def search_web(keyword):\n",
    "    import requests #必要なモジュールのインポート\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    from janome.analyzer import Analyzer\n",
    "    from janome.tokenfilter import POSKeepFilter, TokenCountFilter\n",
    "    \n",
    "\n",
    "    search_url = 'https://www.google.co.jp/search?hl=ja&num=20&q=' + keyword #検索ワードの設定\n",
    "    res_google = requests.get(search_url) #検索の実行\n",
    "    \n",
    "    bs4_google = BeautifulSoup(res_google.text, 'html.parser') #検索結果をhtmlにして取得\n",
    "    search_results = bs4_google.select('.kCrYT > a') #htmlから検索結果一覧の部分を取得(kCrYTは変更されてる可能性あり)\n",
    " \n",
    "    max = 0 #キーワードの出現回数のカウントのための変数\n",
    "\n",
    "    for search_result in search_results:\n",
    "\n",
    "        page_url = re.sub(\"\\/url\\?q=\",\"\",search_result.get('href')) #urlから余計な文字列の削除\n",
    "        page_url = re.sub(\"&sa=U&ved.*\", \"\", page_url)\n",
    "\n",
    "\n",
    "        html = requests.get(page_url) #urlを開いてWebページ情報を取得\n",
    "        soup_page = BeautifulSoup(html.content, \"html.parser\") #Webページからhtmlを取得\n",
    "    \n",
    "        token_filters = [\n",
    "        POSKeepFilter(['名詞']),  # 名詞を抽出するようにする\n",
    "        TokenCountFilter(),  # トークンの出現回数をカウントする\n",
    "        ]\n",
    "        analyzer = Analyzer(token_filters=token_filters)  # フィルターからアナライザーを生成\n",
    "    \n",
    "        try:\n",
    "            word_count = analyzer.analyze(soup_page.find(\"body\").text) #それぞれの名詞の出現回数のカウント\n",
    "        except NameError:\n",
    "            continue #スクレイピング対策などによりページを開けない場合はcontinue\n",
    "    \n",
    "        try:\n",
    "            num = dict(word_count)[keyword] #キーワードの出現回数の取得\n",
    "            if max < int(num): #キーワードの出現回数が、今までの最大値を上回っている場合\n",
    "                max = num #出現回数の最大値を更新\n",
    "                url = page_url #urlの更新\n",
    "                title = search_result.text #タイトルの更新\n",
    "        except KeyError:\n",
    "            continue #キーワードがない場合はcontinue\n",
    "        \n",
    "    return max,title, url #キーワードの出現回数、タイトル、urlを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関数を総括する\n",
    "\n",
    "url = \"https://www.softbank.jp/biz/ai/\" #スクレイピングするURL\n",
    "\n",
    "body = get_body(url) #bodyの抜き出し\n",
    "keyword = extract_keyword(body) #bodyからキーワードの抜き出し\n",
    "result = search_web(keyword) #キーワードから検索、キーワード出現回数のカウント\n",
    "\n",
    "print(\"検索キーワード:\", keyword)\n",
    "print(\"出現回数:\", result[0])\n",
    "print(\"タイトル:\", result[1])\n",
    "print(\"URL:\", result[2])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOHs2GmTJ1uWRyoZMmpjYpc",
   "collapsed_sections": [],
   "name": "2022-04-11_recipie.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
