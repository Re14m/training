{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-0303_recipie120.ipynb","provenance":[],"collapsed_sections":[],"private_outputs":true,"mount_file_id":"1fTVNXGeBJuwoDsuyrON1aV8GA-OxfxdJ","authorship_tag":"ABX9TyMkruoMABOSTb9F+QT9oOab"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"cellView":"code","id":"VLvSqXYjr60C"},"outputs":[],"source":["# パッケージのインストール（追加：形態素解析Sudachi）\n","!pip install sudachipy sudachidict_core"]},{"cell_type":"code","source":["# ライブラリのバージョン確認\n","!pip show sortedcontainers"],"metadata":{"id":"LfrylVo_ztRC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# datasetダウンロード\n","!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n","!tar xzf ldcc-20140209.tar.gz"],"metadata":{"id":"_DGBkFv-sfmz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# datasetの準備\n","import os\n","\n","# テキスト直下のディレクトリ一覧を取得(これがカテゴリになる。)\n","dirlist = os.listdir('text')\n","category_list = {}\n","i=0\n","for dirname in dirlist:\n","    if dirname[-3:] != 'txt':\n","        category_list[str(i)] = dirname\n","        i+=1\n","\n","# データセットを作成して、ファイルに出力する。tsv形式で、ファイル名、ラベルid、カテゴリ名、テキストを出力する。\n","with open('dataset.tsv', 'w') as f_out:\n","    for label, category in category_list.items():\n","        path = './text/{}/'.format(category)\n","        filelist = os.listdir(path)\n","        filelist.remove('LICENSE.txt')\n","        for filename in filelist:\n","            with open(path + filename, 'r') as f_in:\n","                # テキストはタイトルのみ取得　(本文は学習対象にしない)\n","                text = f_in.readlines()[2]\n","                # カラム生成\n","                out_row = [filename, label, category, text]\n","                f_out.write(\"\\t\".join(out_row))"],"metadata":{"id":"dsmNoShJsq4V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataset.tsvをpandasでロード\n","import pandas as pd\n","df = pd.read_table(\n","    'dataset.tsv',\n","    names=['filename', 'label', 'category', 'text']\n","    ).sample(frac=1, random_state=0).reset_index(drop=True)"],"metadata":{"id":"OiM7s2uxtMGn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# データを学習:テスト用=8:2に分割\n","N = len(df)\n","train_df = df[:int(N * 0.8)] # 学習\n","test_df = df[int(N * 0.8):] # テスト"],"metadata":{"id":"LWye4TASt8Ke"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ベースライン決定\n","from unicodedata import normalize\n","from sudachipy import tokenizer\n","from sudachipy import dictionary\n","import string\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# 前処理\n","class TextPreprocessing(object):\n","    def __init__(self):\n","        self.tokenizer_obj = dictionary.Dictionary().create()\n","        self.mode = tokenizer.Tokenizer.SplitMode.A\n","        self.vectorizer = CountVectorizer()\n","\n","    # テキストに対して前処理を実施\n","    def _preprocess(self, text):\n","        # トークン化\n","        morphs = []\n","        for m in self.tokenizer_obj.tokenize(text, self.mode):\n","            morphs.append(m.surface())\n","\n","        return \" \".join(morphs)\n","\n","    # 文章データの行列を生成(各文章に対するベクトル一覧)\n","    def get_matrix(self, text_series, mode='train'):\n","        text_series = text_series.map(self._preprocess)\n","\n","        if mode == 'train':\n","            # 辞書作成と文章データの行列を作成\n","            bag = self.vectorizer.fit_transform(text_series)\n","        else:\n","            # 文章データの行列を作成 ※ 辞書はtrainでつくったものを使用\n","            bag = self.vectorizer.transform(text_series)\n","\n","        return bag"],"metadata":{"id":"S4t6k2Xit-qr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ベースラインの評価\n","%%time\n","# 学習データに対する前処理\n","tp = TextPreprocessing()\n","bag = tp.get_matrix(train_df.text)\n","train_X = bag.toarray()\n","train_y = pd.Series(train_df.label)"],"metadata":{"id":"nsoynp0i0wN_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ベースラインの評価\n","%%time\n","# 学習\n","from sklearn.linear_model import LogisticRegression\n","\n","clf = LogisticRegression()\n","clf.fit(train_X, train_y)"],"metadata":{"id":"I2l0_WU22Ojq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ベースラインの評価\n","# 正解率\n","bag_test = tp.get_matrix(test_df.text, mode='test')\n","test_X = bag_test.toarray()\n","test_y = pd.Series(test_df.label)\n","score = clf.score(test_X, test_y)\n","print(score)"],"metadata":{"id":"YZ0qDukn2Pnx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# スパース行列の確認\n","print(train_X)"],"metadata":{"id":"kBnYoiVgzIKc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# スパース行列を利用しての処理速度改善\n","%%time\n","from scipy.sparse import lil_matrix\n","from sklearn.linear_model import LogisticRegression\n","\n","train_X = lil_matrix(train_X) # スパース行列に変換\n","\n","clf = LogisticRegression()\n","clf.fit(train_X, train_y)"],"metadata":{"id":"pzeI59vQzVfY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 前処理改善\n","from unicodedata import normalize\n","import string\n","from sudachipy import tokenizer\n","from sudachipy import dictionary\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# 前処理\n","class TextPreprocessing(object):\n","    def __init__(self):\n","        self.tokenizer_obj = dictionary.Dictionary().create()\n","        self.mode = tokenizer.Tokenizer.SplitMode.A\n","        punctuation = string.punctuation + '。、×÷ 【】『』 「」”“'\n","        self.noises = str.maketrans(\n","            {k: ' ' for k in normalize('NFKC', punctuation)})\n","        self.vectorizer = CountVectorizer()\n","\n","    # ユニコード正規化を実施したうえで、トークン化を実施\n","    def _preprocess(self, text):\n","        # unicode正規化と記号除去\n","        text = normalize('NFKC', text).lower()\n","        text = text.translate(self.noises).strip()\n"," \n","        # トークン化\n","        morphs = []\n","        for m in self.tokenizer_obj.tokenize(text, self.mode):\n","            if m.part_of_speech()[0] == '名詞' and m.part_of_speech()[1] != '数詞':\n","                morphs.append(m.surface())\n","        return \" \".join(morphs)\n","\n","    # 文章データの行列を生成(各文章に対するベクトル一覧)\n","    def get_matrix(self, text_series, mode='train'):\n","        text_series = text_series.map(self._preprocess)\n","        if mode == 'train':\n","            # 辞書作成と文章データの行列を作成\n","            bag = self.vectorizer.fit_transform(text_series)\n","        else:\n","            # 文章データの行列を作成 ※ 辞書はtrainでつくったものを使用\n","            bag = self.vectorizer.transform(text_series)\n","\n","        return bag"],"metadata":{"id":"X66gXNNdzlFN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 前処理改善の評価\n","%%time\n","# 学習データに対する前処理\n","tp = TextPreprocessing()\n","bag = tp.get_matrix(train_df.text)\n","train_X = bag.toarray()\n","train_y = pd.Series(train_df.label)"],"metadata":{"id":"IZLjErrZzp8S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 前処理改善の評価\n","%%time\n","# 学習時間\n","from scipy.sparse import lil_matrix\n","from sklearn.linear_model import LogisticRegression\n","\n","clf = LogisticRegression()\n","clf.fit(lil_matrix(train_X), train_y)"],"metadata":{"id":"mMTXI6m1zs8J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 前処理改善の評価\n","# 正解率\n","bag_test = tp.get_matrix(test_df.text, mode='test')\n","test_X = bag_test.toarray()\n","test_y = pd.Series(test_df.label)\n","score = clf.score(lil_matrix(test_X), test_y)\n","print(score)"],"metadata":{"id":"AkeLD1cEzxZd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 前処理改善（語彙数の確認）\n","d = tp.vectorizer.vocabulary_\n","print(len(d)) # out -> 9463"],"metadata":{"id":"ZiylXNpdz3cs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 前処理改善（語彙を減らす）\n","from unicodedata import normalize\n","import string\n","from sudachipy import tokenizer\n","from sudachipy import dictionary\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# 前処理\n","class TextPreprocessing(object):\n","    def __init__(self, max_features=None):\n","        self.tokenizer_obj = dictionary.Dictionary().create()\n","        self.mode = tokenizer.Tokenizer.SplitMode.A\n","        punctuation = string.punctuation + '。、×÷ 【】『』 「」”“'\n","        self.noises = str.maketrans(\n","            {k: ' ' for k in normalize('NFKC', punctuation)})\n","        # max_featuresを追加\n","        self.vectorizer = CountVectorizer(max_features = max_features)\n","\n","    # ユニコード正規化を実施したうえで、トークン化を実施\n","    def _preprocess(self, text):\n","        # unicode正規化とノイズ除去\n","        text = normalize('NFKC', text).lower()\n","        text = text.translate(self.noises).strip()\n","\n","        # トークン化\n","        morphs = []\n","        for m in self.tokenizer_obj.tokenize(text, self.mode):\n","            if m.part_of_speech()[0] == '名詞' and m.part_of_speech()[1] != '数詞':\n","                morphs.append(m.surface())\n","        return \" \".join(morphs)\n","\n","    # 文章データの行列を生成(各文章に対するベクトル一覧)\n","    def get_matrix(self, text_series, mode='train'):\n","        text_series = text_series.map(self._preprocess)\n","        if mode == 'train':\n","            # 辞書作成と文章データの行列を作成\n","            bag = self.vectorizer.fit_transform(text_series)\n","        else:\n","            # 文章データの行列を作成 ※ 辞書はtrainでつくったものを使用\n","            bag = self.vectorizer.transform(text_series)\n","\n","        return bag"],"metadata":{"id":"zpZ7KfOFz8BU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 正解率の変化\n","from sklearn.linear_model import LogisticRegression\n","import matplotlib.pyplot as plt\n","\n","candidate = list(range(1000, 11000, 1000))\n","clf = LogisticRegression()\n","scores = []\n","for max_features in candidate:\n","    tp = TextPreprocessing(max_features=max_features)\n","    bag = tp.get_matrix(train_df.text)\n","    train_X = bag.toarray()\n","    train_y = pd.Series(train_df.label)\n","    clf.fit(lil_matrix(train_X), train_y)\n","\n","    bag_test = tp.get_matrix(test_df.text, mode='test')\n","    test_X = bag_test.toarray()\n","    test_y = pd.Series(test_df.label)\n","    scores.append(clf.score(lil_matrix(test_X), test_y))\n","\n","plt.plot(candidate, scores, label='socre')\n","plt.legend()"],"metadata":{"id":"EKaOgoM20G0L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 評価\n","# 上のグラフから7500あたりが頭打ちになっていることがわかるのでmax_featuresに7500を代入\n","%%time\n","# 学習データに対する前処理\n","tp = TextPreprocessing(max_features=7500)\n","bag = tp.get_matrix(train_df.text)\n","train_X = bag.toarray()\n","train_y = pd.Series(train_df.label)"],"metadata":{"id":"im0mUqKJ0VFp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 評価\n","%%time\n","# 学習時間\n","from scipy.sparse import lil_matrix\n","from sklearn.linear_model import LogisticRegression\n","\n","clf = LogisticRegression()\n","clf.fit(lil_matrix(train_X), train_y)"],"metadata":{"id":"l5oZfKmG0etR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 評価\n","# 正解率\n","bag_test = tp.get_matrix(test_df.text, mode='test')\n","test_X = bag_test.toarray()\n","test_y = pd.Series(test_df.label)\n","score = clf.score(lil_matrix(test_X), test_y)\n","print(score)"],"metadata":{"id":"gT3rxFOZ0f69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ハイパーパラメータのチューニング\n","import matplotlib.pyplot as plt\n","\n","C_candidate = [0.5, 1.0, 1.5, 2.0]\n","scores = []\n","train_X=lil_matrix(train_X)\n","test_X=lil_matrix(test_X)\n","\n","for c in C_candidate:\n","    clf = LogisticRegression(C=c)\n","    clf.fit(train_X, train_y)\n","    scores.append(clf.score(test_X, test_y))\n","\n","plt.plot(C_candidate, scores, label='score')\n","plt.legend()"],"metadata":{"id":"I6k7_BdC0425"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ハイパーパラメータのチューニング\n","import matplotlib.pyplot as plt\n","\n","iter_candidate = [30, 40, 50, 60, 70]\n","scores = []\n","train_X=lil_matrix(train_X)\n","test_X=lil_matrix(test_X)\n","\n","for iter in iter_candidate:\n","    clf = LogisticRegression(max_iter=iter)\n","    clf.fit(train_X, train_y)\n","    scores.append(clf.score(test_X, test_y))\n","\n","plt.plot(iter_candidate, scores, label='score')\n","plt.legend()"],"metadata":{"id":"8uRZrfWi1VTA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# グリッドサーチによるチューニング\n","import itertools\n","import warnings\n","\n","warnings.simplefilter('ignore')\n","train_X=lil_matrix(train_X)\n","test_X=lil_matrix(test_X)\n","\n","# グリッドサーチによるパラメーター探索を実装した関数\n","def search_best_param(grid_param,  estimator, verbose=False):\n","    best_acc = 0\n","    best_param = {}\n","    product = [x for x in itertools.product(*grid_param.values())]\n","    params = [dict(zip(grid_param.keys(), r)) for r in product]\n","    for param in params:\n","        estimator.set_params(**param)\n","        estimator.fit(train_X, train_y)\n","        acc = clf.score(test_X, test_y)\n","        if verbose:\n","            print(param, acc)\n","        if  acc > best_acc:\n","            best_acc = acc\n","            best_param = param\n","    return best_param, best_acc\n","\n","candidate = []\n","# 探索するパラメーターの組み合わせ\n","grid_param = {\n","    \"C\": [1.3, 1.4, 1.5, 1.6, 1.7],\n","    \"max_iter\": [35, 40, 45]\n","    }\n","clf = LogisticRegression()\n","candidate.append(search_best_param(grid_param, clf))\n","\n","best_param, best_score = sorted(candidate, key=lambda x: x[1], reverse=True)[0]\n","print(best_param, best_score) # 最大スコアを出したパラメーターとスコア"],"metadata":{"id":"g8Jt0DNW1x8o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 評価\n","%%time\n","# 学習時間\n","from scipy.sparse import lil_matrix\n","from sklearn.linear_model import LogisticRegression\n","\n","clf = LogisticRegression(**best_param) ## グリッドサーチで選択したベストパラムを指定\n","clf.fit(lil_matrix(train_X), train_y)"],"metadata":{"id":"tI7VpvE82mUO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 評価\n","# 正解率\n","bag_test = tp.get_matrix(test_df.text, mode='test')\n","test_X = bag_test.toarray()\n","test_y = pd.Series(test_df.label)\n","score = clf.score(lil_matrix(test_X), test_y)\n","print(score)"],"metadata":{"id":"kmr4Xkfs2pXA"},"execution_count":null,"outputs":[]}]}