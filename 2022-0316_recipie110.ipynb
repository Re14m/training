{"cells":[{"cell_type":"markdown","source":["<a href=\"https://colab.research.google.com/github/Re14m/training/blob/master/2022-0316_recipie110.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"],"metadata":{"id":"9tSo0z2UyjbM"}},{"cell_type":"markdown","source":["# [文書の「あいまい検索」機能をつくるレシピ](https://axross-recipe.com/recipes/110)"],"metadata":{"id":"jeEOivIHdMEG"}},{"cell_type":"markdown","metadata":{"id":"jkQuMK3Dhp2Q"},"source":["ローカル環境で実施しました。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ft5RZUUdX2rc","scrolled":true},"outputs":[],"source":["# パッケージのインストール\n","!pip install numpy\n","!pip install gensim\n","!pip install mecab-python3\n","!pip install neologdn\n","!pip install tqdm\n","\n","# Mecab用の辞書のインストール\n","!pip install unidic-lite"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R6fkEh6lSNbw"},"outputs":[],"source":["# dataset DL (https://www.rondhuit.com/download/ldcc-20140209.tar.gz)\n","# dataset 解凍\n","from shutil import unpack_archive\n","unpack_archive(filename=\"/ldcc-20140209.tar.gz\", extract_dir=\"/data/\", format=\"gztar\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QfMQbFm1UdaM"},"outputs":[],"source":["# dataset 読込\n","from pathlib import Path\n","doc_dir = Path(\"/data/text/\")\n","doc_paths = []\n","for d in doc_dir.iterdir():\n","    if d.is_dir():\n","        docs = d.glob(\"*.txt\")\n","        doc_paths += list(docs)\n","print(\"ニュース記事数:\", len(doc_paths))\n","print(doc_paths[:10])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"objBWFj1W3BN"},"outputs":[],"source":["# ファイルパスからニュース記事を表示する\n","def read_doc(path):\n","    with open(path,encoding=\"utf-8\") as f:\n","        doc = f.read()\n","    return doc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u2V0ULiHXkCU"},"outputs":[],"source":["# 出力\n","print(read_doc(doc_paths[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7m2-iN3X-A8"},"outputs":[],"source":["# a.Mecabの動作テスト\n","import MeCab\n","mecab = MeCab.Tagger('-Ochasen')\n","data = mecab.parse('庭には２羽裏庭には２羽鶏がいる')\n","print(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vry977kecRDQ"},"outputs":[],"source":["# b.品詞を動詞,名詞,形容詞に限定して分ち書き\n","test = \"庭には２羽裏庭には２羽鶏がいる\"\n","\n","l = [line.split(\"\\t\") for line in mecab.parse(test).split(\"\\n\")]\n","res = []\n","for w in l:\n","    if len(w) >=4: # check nomal words (e.g. not EOS)\n","        pos = w[3]\n","        base = w[2]\n","        group_pos = pos.split(\"-\")[0]\n","        if group_pos in [\"動詞\",\"名詞\",\"形容詞\"]:\n","            res.append(base)\n","print(res)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IdiY8bRkcz7e"},"outputs":[],"source":["# c.stopwordを表示（ノイズ除去用）\n","from urllib import request\n","res = request.urlopen(\"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt\")\n","stopwords = [line.decode(\"utf-8\").strip() for line in res if len(line.strip()) > 0]\n","print(\"ストップワード数:\", len(stopwords))\n","print(stopwords)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LrdT5Wl7dFeA"},"outputs":[],"source":["# d.stopwordを適用する\n","test = \"庭には２羽裏庭には２羽鶏がいる\"\n","\n","l = [line.split(\"\\t\") for line in mecab.parse(test).split(\"\\n\")]\n","res = []\n","for w in l:\n","    if len(w) >=4: # check nomal words (e.g. not EOS)\n","        pos = w[3]\n","        group_pos = pos.split(\"-\")[0]\n","        base = w[2]\n","        if group_pos in [\"動詞\",\"名詞\",\"形容詞\"] and base not in stopwords: \n","            res.append(base)\n","print(res)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2B1EtrycdXOk"},"outputs":[],"source":["# a-dを文章が変わっても利用できるようにクラス化する\n","class Tokenizer:\n","    def __init__(self, stopwords=None, include_pos=None):\n","        tagger_cmd = \"-Ochasen\"\n","        mecab = MeCab.Tagger(tagger_cmd)\n","        self.parser = mecab.parse\n","        if stopwords is None:\n","            self.stopwords = []\n","        else:\n","            self.stopwords = stopwords\n","        if include_pos is None:\n","            self.include_pos = [\"名詞\", \"動詞\", \"形容詞\"]\n","        else:\n","            self.include_pos = include_pos\n","\n","    def tokenize(self, text):\n","        l = [line.split(\"\\t\") for line in self.parser(text).split(\"\\n\")]\n","        res = []\n","        for w in l:\n","            if len(w) >=4: # check nomal words (e.g. not EOS)\n","                pos = w[3]\n","                group_pos = pos.split(\"-\")[0]\n","                base = w[2]\n","                if group_pos in self.include_pos and base not in self.stopwords:\n","                    res.append(base)\n","        return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cq-vqr3UdzG0"},"outputs":[],"source":["# tokenizerクラスの出力結果\n","res = request.urlopen(\"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt\")\n","stopwords = [line.decode(\"utf-8\").strip() for line in res if len(line.strip()) > 0]\n","\n","include_pos = [\"名詞\", \"動詞\", \"形容詞\"]\n","\n","tokenizer = Tokenizer(stopwords=stopwords, include_pos=include_pos)\n","\n","words = tokenizer.tokenize(\"庭には２羽裏庭には２羽鶏がいる\")\n","print(words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xSaDFAsteEMZ"},"outputs":[],"source":["# datasetをtokenizerにかける（nomalize）\n","import pprint\n","words = tokenizer.tokenize(read_doc(doc_paths[0]))\n","pprint.pprint(words, compact=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"atFkVIW5es_q"},"outputs":[],"source":["# 正規化する\n","import neologdn\n","normalized_word = neologdn.normalize(\"元気～～～？？？\") #複数の伸ばし棒\n","print(normalized_word)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0h0PGKYfe6MQ"},"outputs":[],"source":["# 正規化のための関数を用意する\n","import re\n","def normalize(text):\n","    text = re.sub(r\"http(s)?:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*\", \" \", text) # remove web urls\n","    text = re.sub(r\"(\\d{4})-(\\d{2})-(\\d{2})T(\\d{2}):(\\d{2}):(\\d{2})\\+0900\", \"\", text) # remove time\n","    text = re.sub(r\"\\\"?([-a-zA-Z0-9.`?{}]+\\.jp)\\\"?\" ,\" \", text) # remove .jp urls\n","    text = re.sub(r'\\d+', ' ', text) # remove disits\n","    text = re.sub(r\"[\\(\\),.:=\\?？！\\+><\\[\\]\\|\\\"\\';\\n【】『』\\-\\!、。“■（）]\", \" \", text) # remove marks\n","    text = neologdn.normalize(text) \n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ur6nmRl1fHAg"},"outputs":[],"source":["# 抽出した文字列を更に正規化する\n","normalized_word =normalize(read_doc(doc_paths[0]))\n","print(normalized_word)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BRsVTsfPfWB1"},"outputs":[],"source":["# 正規化した後でtokenizerにかける\n","words = tokenizer.tokenize(normalize(read_doc(doc_paths[0])))\n","pprint.pprint(words, compact=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ESiwkpXLf22_"},"outputs":[],"source":["# ベクトル化（fasttext）\n","# モデル作成\n","from gensim.models.fasttext import FastText\n","model = FastText(vector_size=300) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7eby_8Kdf-38"},"outputs":[],"source":["# datasetすべての文字列から必要な単語だけを抽出してリスト化\n","from tqdm.notebook import tqdm\n","sentences = [tokenizer.tokenize(normalize(read_doc(p))) for p in tqdm(doc_paths)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fvgS8je5gIeM"},"outputs":[],"source":["# fasttextで学習\n","model.build_vocab(sentences)\n","model.train(sentences, total_examples=len(sentences), epochs=30)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NXzx13iNgjE6"},"outputs":[],"source":["# 単語のベクトルを見る\n","vector = model.wv[\"猫\"] \n","print(\"次元:\", vector.shape)\n","pprint.pprint(vector, compact=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tym1ca-JhHrX"},"outputs":[],"source":["# sentenceからベクトルを生成するリスト作る\n","import numpy as np\n","def to_vec(sentence):\n","    return np.mean([model.wv[w] for w in sentence], axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dKRzygmTYQYZ"},"outputs":[],"source":["doc_vec = to_vec(sentences[0])\n","pprint.pprint(doc_vec, compact=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"liJL0cnxYQYa"},"outputs":[],"source":["# 検索機能の作成\n","# ベクトル類似度=コサイン類似度\n","def cos_sim(v1, v2):\n","    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z4fq_-6sYQYa"},"outputs":[],"source":["v1 = [1,0] # →\n","\n","v2 = [-1,0] # ← (v1と逆向き)\n","v3 = [0, 1] # ↑ (v1と直角)\n","v4 = [0.8,0.2] # v1と向きが近い\n","print(\"v1とv2の類似度:\", cos_sim(v1,v2))\n","print(\"v1とv3の類似度:\", cos_sim(v1,v3))\n","print(\"v1とv4の類似度:\", cos_sim(v1,v4))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FMZE-EOXYQYb"},"outputs":[],"source":["# 作成したセンテンスをすべてベクトル化\n","vecs_with_idx = [(idx, to_vec(s)) for idx, s in enumerate(sentences)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yoKH5-UDYQYb"},"outputs":[],"source":["# 類似度の高い記事を取得する関数を作成\n","def get_similaries(target_vec, vecs_with_idx, topn=10):\n","    sim_list = [(idx, cos_sim(target_vec, v)) for idx, v in vecs_with_idx]\n","    result = sorted(sim_list, key=lambda t: t[1], reverse=True)\n","    return result[:topn]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vNDdqo6HYQYc"},"outputs":[],"source":["# 機能の確認\n","search_words = [\"ビジネスマン\"]\n","target_vec = to_vec(search_words) \n","res = get_similaries(target_vec, vecs_with_idx, topn=10)\n","print(res)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"atu9bPo3YQYd"},"outputs":[],"source":["# 結果の文書を閲覧する（1番目）\n","best_index = res[0][0]\n","print(\"ファイルパス:\", doc_paths[best_index])\n","print(read_doc(doc_paths[best_index]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pus_wn5vYQYd"},"outputs":[],"source":["# 結果の文書を閲覧する（2番目）\n","secondary_index = res[1][0]\n","print(\"ファイルパス:\", doc_paths[secondary_index])\n","print(read_doc(doc_paths[secondary_index]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vV0i5N28YQYe"},"outputs":[],"source":["# <応用> 類似した記事を出す\n","target_doc_index = 1\n","print(\"ファイルパス:\", doc_paths[target_doc_index])\n","print(read_doc(doc_paths[target_doc_index])) #対象の記事を表示\n","target_vec = vecs_with_idx[target_doc_index][1] #対象の記事のベクトル\n","res = get_similaries(target_vec, vecs_with_idx, topn=10)\n","print(res)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NEruWRp3YQYe"},"outputs":[],"source":["# 結果の文書を閲覧する（１番目）\n","best_index = res[1][0] # 0 0は上記の検索結果文書自体を指している\n","print(\"ファイルパス:\", doc_paths[best_index])\n","print(read_doc(doc_paths[best_index]))"]},{"cell_type":"markdown","source":["おまけのため、今回は環境を作らずに手順のみ記載"],"metadata":{"id":"k7GaXmqcYVDW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4hAXXsj2YQYf"},"outputs":[],"source":["# おまけ（検索精度を上げるための方法）①\n","\n","# https://github.com/neologd/mecab-ipadic-neologdからDL\n","# neologd利用\n","text = \"猫ひろし\"\n","MeCab.Tagger('-Ochasen -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n","result = tagger.parse(text)\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W42UAPoZYQYg"},"outputs":[],"source":["# ochasen利用\n","tagger_cmd = \"-Ochasen\" \n","print(mecab.parse(\"猫ひろし\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"itFVMJE4YQYg"},"outputs":[],"source":["# おまけ（検索精度を上げるための方法）②\n","# https://fasttext.cc/docs/en/crawl-vectors.htmlからDL\n","# fasttextの学習済みモデルの利用\n","from gensim.models.fasttext import load_facebook_model\n","model = load_facebook_model(\"cc.ja.300.bin.gz\")\n","model.build_vocab(sentences=sentences, update=True)\n","model.train(sentences=sentences, total_examples=len(sentences), epochs=30)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"2022-0316_recipie110.ipynb","provenance":[],"private_outputs":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}