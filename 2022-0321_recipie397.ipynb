{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-0321_recipie397.ipynb","provenance":[],"collapsed_sections":[],"private_outputs":true,"mount_file_id":"1fTVNXGeBJuwoDsuyrON1aV8GA-OxfxdJ","authorship_tag":"ABX9TyPI/SdyrQ6itE49ZSdGWd/J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["<a href=\"https://colab.research.google.com/github/Re14m/training/blob/master/2022-0321_recipie397.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"],"metadata":{"id":"5PbKRJE0yVrn"}},{"cell_type":"markdown","source":["# [ゼロからword2vecを学習して可視化するレシピ](https://axross-recipe.com/recipes/397)"],"metadata":{"id":"68lbmC9reG6j"}},{"cell_type":"code","source":["#パッケージのインストール\n","!pip install tensorflow"],"metadata":{"id":"OvMZTbLUwQgA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# コーパス\n","corpus = [\n","    \"dog is a canine\",\n","    \"canine is a mammal\",\n","    \"mammal is an animal\",\n","    \"animal is a living thing\",\n","    \"living thing is a thing\",\n","    \"bulldog is a dog\",\n","    \"kitty is a cat\",\n","    \"cat is a mammal\",\n","]"],"metadata":{"id":"rfIDXWONVdVl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# テキストの前処理を実施（stopwords）\n","def remove_stop_words(corpus):\n","    stop_words = [\"is\", \"a\", \"an\", \"will\", \"be\"]\n","    results = []\n","    for text in corpus:\n","        tmp = text.split(\" \")\n","        for stop_word in stop_words:\n","            if stop_word in tmp:\n","                tmp.remove(stop_word)\n","        results.append(\" \".join(tmp))\n","    return results\n","\n","corpus = remove_stop_words(corpus)\n","corpus"],"metadata":{"id":"nvAM8M6mVg-Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 単語ごとに分割（stopwords）\n","words = []\n","for text in corpus:\n","    for word in text.split(\" \"):\n","        words.append(word)\n","\n","words = set(words)\n","words"],"metadata":{"id":"yYxZequZV7DF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 近傍に出現する単語のカウント（Skip-Gram）\n","word_to_id = {}\n","for i, word in enumerate(words):\n","    word_to_id[word] = i\n","\n","sentences = []\n","for sentence in corpus:\n","    sentences.append(sentence.split())\n","\n","window_size = 2\n","\n","data = []\n","for sentence in sentences:\n","    for idx, word in enumerate(sentence):\n","        for neighbor in sentence[\n","            max(idx - window_size, 0) : min(idx + window_size, len(sentence)) + 1\n","        ]:\n","            if neighbor != word:\n","                data.append([word, neighbor])\n","\n","import pandas as pd\n","\n","df = pd.DataFrame(data, columns=[\"input\", \"label\"])\n","print(df)"],"metadata":{"id":"5GbpC-88YxdU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# datasetの作成\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","\n","import numpy as np\n","\n","ONE_HOT_DIM = len(words)\n","\n","def to_one_hot_encoding(data_point_index):\n","    one_hot_encoding = np.zeros(ONE_HOT_DIM)\n","    one_hot_encoding[data_point_index] = 1\n","    return one_hot_encoding\n","\n","X = []\n","Y = []\n","\n","for x, y in zip(df[\"input\"], df[\"label\"]):\n","    X.append(to_one_hot_encoding(word_to_id[x]))\n","    Y.append(to_one_hot_encoding(word_to_id[y]))\n","\n","X_train = np.asarray(X)\n","Y_train = np.asarray(Y)\n","\n","print(word_to_id)\n","print(X_train)\n","print(Y_train)"],"metadata":{"id":"mmWw-BJfZCOW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# モデル構築・学習\n","x = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n","y_label = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n","EMBEDDING_DIM = 2\n","\n","w1 = tf.Variable(tf.random_normal([ONE_HOT_DIM, EMBEDDING_DIM]))\n","b1 = tf.Variable(tf.random_normal([1]))\n","hidden_layer = tf.add(tf.matmul(x, w1), b1)\n","\n","w2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, ONE_HOT_DIM]))\n","b2 = tf.Variable(tf.random_normal([1]))\n","prediction = tf.nn.softmax(tf.add(tf.matmul(hidden_layer, w2), b2))\n","\n","loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), axis=[1]))\n","\n","train_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n","\n","sess = tf.Session()\n","init = tf.global_variables_initializer()\n","sess.run(init)\n","\n","iteration = 20000\n","for i in range(iteration):\n","    sess.run(train_op, feed_dict={x: X_train, y_label: Y_train})\n","    if i % 3000 == 0:\n","        print(\n","            \"iteration \" + str(i) + \" loss is : \",\n","            sess.run(loss, feed_dict={x: X_train, y_label: Y_train}),\n","        )\n","vectors = sess.run(w1 + b1)\n","w2v_df = pd.DataFrame(vectors, columns=[\"x1\", \"x2\"])\n","w2v_df[\"word\"] = list(words)\n","w2v_df = w2v_df[[\"word\", \"x1\", \"x2\"]]\n","w2v_df"],"metadata":{"id":"LaH-paeswxpy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 可視化（word2vec）\n","import matplotlib.pyplot as plt\n","\n","fig, ax = plt.subplots()\n","\n","for word, x1, x2 in zip(w2v_df[\"word\"], w2v_df[\"x1\"], w2v_df[\"x2\"]):\n","    ax.annotate(word, (x1, x2))\n","\n","PADDING = 1.0\n","\n","x_axis_min = np.amin(vectors, axis=0)[0] - PADDING\n","y_axis_min = np.amin(vectors, axis=0)[1] - PADDING\n","x_axis_max = np.amax(vectors, axis=0)[0] + PADDING\n","y_axis_max = np.amax(vectors, axis=0)[1] + PADDING\n","\n","plt.xlim(x_axis_min, x_axis_max)\n","plt.ylim(y_axis_min, y_axis_max)\n","plt.rcParams[\"figure.figsize\"] = (10, 10)\n","\n","plt.show()"],"metadata":{"id":"3t-fBKiByVdQ"},"execution_count":null,"outputs":[]}]}